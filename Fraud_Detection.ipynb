{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**SUMMARY**:\n",
        "\n",
        "The capstone project focuses on Fraud Detection Analysis in Data Science, covering various important aspects of the project lifecycle. Here is a brief overview based on the provided context:\n",
        "\n",
        "1. Dataset: The project involves working with a specific dataset for fraud detection analysis.\n",
        "\n",
        "2. Project Objectives and Scope: Key questions include defining success for the fraud detection model and anticipating challenges during the model's implementation.\n",
        "\n",
        "3. Data Analysis: Exploring how to handle multicollinearity in the dataset and the significance of feature importance in the analysis process.\n",
        "\n",
        "4. Data Preprocessing: Techniques for dealing with categorical data and the significance of splitting the dataset into training and testing sets for model development.\n",
        "\n",
        "5. Model Training: Understanding the differences between Gaussian Naive Bayes and other variants, and strategies for optimizing model parameters.\n",
        "\n",
        "6. Model Evaluation: Ensuring the reliability of evaluation metrics and understanding the impact of false positives and false negatives in fraud detection.\n",
        "\n",
        "7. Results and Interpretation: Validating model results and discussing the implications of the findings for stakeholders.\n",
        "\n",
        "8. Model Improvement: Incorporating domain knowledge into the model and considering advanced techniques like ensemble methods for enhancing performance.\n",
        "\n",
        "9. Practical Implementation: Ensuring scalability of the fraud detection model and addressing data privacy and security concerns.\n",
        "\n",
        "10. Technical Implementation: Managing dependencies and version control for implementation and utilizing pipelines in machine learning workflows for efficiency.\n",
        "\n",
        "Each section plays a crucial role in developing a robust fraud detection model, from data analysis to model training, evaluation, and practical implementation. The project aims to leverage various techniques and strategies to achieve accurate and reliable fraud detection results while addressing key challenges and ensuring the model's scalability and security.  "
      ],
      "metadata": {
        "id": "E6ql7uFt0ps2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n"
      ],
      "metadata": {
        "id": "phWH-aUTzRaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv('/content/creditcard.csv')"
      ],
      "metadata": {
        "id": "Jv67-Wkz4uhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data** **Analysis**"
      ],
      "metadata": {
        "id": "hviCvOykD_lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle multicollinearity\n",
        "corr_matrix = df.corr()\n",
        "print(corr_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwDBG2sB_Ncx",
        "outputId": "735183da-ac03-46b6-d70d-9ab27f5fc69f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Time            V1            V2            V3            V4  \\\n",
            "Time    1.000000  1.173963e-01 -1.059333e-02 -4.196182e-01 -1.052602e-01   \n",
            "V1      0.117396  1.000000e+00  3.777823e-12 -2.118614e-12 -1.733159e-13   \n",
            "V2     -0.010593  3.777823e-12  1.000000e+00  2.325661e-12 -2.314981e-12   \n",
            "V3     -0.419618 -2.118614e-12  2.325661e-12  1.000000e+00  2.046235e-13   \n",
            "V4     -0.105260 -1.733159e-13 -2.314981e-12  2.046235e-13  1.000000e+00   \n",
            "V5      0.173072 -3.473231e-12 -1.831952e-12 -4.032993e-12 -2.552389e-13   \n",
            "V6     -0.063016 -1.306165e-13  9.438444e-13 -1.574471e-13  1.084041e-12   \n",
            "V7      0.084714 -1.116494e-13  5.403436e-12  3.405586e-12  8.135064e-13   \n",
            "V8     -0.036949  2.114527e-12  2.133785e-14 -1.272385e-12  7.334818e-13   \n",
            "V9     -0.008660  3.016285e-14  3.238513e-13 -6.812351e-13 -7.143069e-13   \n",
            "V10     0.030617 -2.615192e-12  1.463282e-12 -1.609126e-12 -1.938143e-12   \n",
            "V11    -0.247689  1.866551e-12 -8.314960e-13  8.707055e-13  1.874473e-12   \n",
            "V12     0.124348 -1.238745e-12  6.139448e-13 -2.730043e-12  5.393827e-13   \n",
            "V13    -0.065902  7.589589e-13 -1.181068e-12 -1.020592e-12  6.813810e-13   \n",
            "V14    -0.098757 -1.871054e-13 -3.384684e-13 -5.597874e-13 -1.404120e-12   \n",
            "V15    -0.183453 -3.601390e-13  2.196083e-13  6.442512e-13  1.526382e-12   \n",
            "V16     0.011903 -1.142884e-12 -8.000510e-13 -8.748795e-13  3.095722e-13   \n",
            "V17    -0.073297  1.671073e-12  2.028957e-12 -1.058101e-12  1.151414e-14   \n",
            "V18     0.090438 -5.738830e-13 -1.916566e-14 -8.846578e-13 -1.309615e-12   \n",
            "V19     0.028975 -2.770259e-12 -2.237098e-13 -1.061131e-12 -9.754131e-13   \n",
            "V20    -0.050866  2.662926e-13  5.839893e-13  1.873059e-12 -2.347029e-12   \n",
            "V21     0.044736 -3.276238e-12  2.280202e-12  6.736294e-13 -2.696370e-12   \n",
            "V22     0.144059  2.281863e-12 -2.548560e-13 -8.909339e-13  4.347776e-13   \n",
            "V23     0.051142 -2.969746e-12 -4.856120e-12  4.147209e-12 -4.160969e-12   \n",
            "V24    -0.016182 -1.029876e-12  6.431308e-13  3.407636e-12 -2.368743e-12   \n",
            "V25    -0.233083  1.144179e-12 -9.423730e-13  5.712956e-13  1.619944e-12   \n",
            "V26    -0.041407  1.835263e-12 -4.129100e-13 -2.577274e-12 -3.043100e-13   \n",
            "V27    -0.005135  7.624804e-12 -9.856545e-13 -5.041444e-12 -1.456066e-12   \n",
            "V28    -0.009413 -9.769215e-13  2.525513e-12  5.189109e-12 -2.832372e-12   \n",
            "Amount -0.010596 -2.277087e-01 -5.314089e-01 -2.108805e-01  9.873167e-02   \n",
            "Class  -0.012323 -1.013473e-01  9.128865e-02 -1.929608e-01  1.334475e-01   \n",
            "\n",
            "                  V5            V6            V7            V8            V9  \\\n",
            "Time    1.730721e-01 -6.301647e-02  8.471437e-02 -3.694943e-02 -8.660434e-03   \n",
            "V1     -3.473231e-12 -1.306165e-13 -1.116494e-13  2.114527e-12  3.016285e-14   \n",
            "V2     -1.831952e-12  9.438444e-13  5.403436e-12  2.133785e-14  3.238513e-13   \n",
            "V3     -4.032993e-12 -1.574471e-13  3.405586e-12 -1.272385e-12 -6.812351e-13   \n",
            "V4     -2.552389e-13  1.084041e-12  8.135064e-13  7.334818e-13 -7.143069e-13   \n",
            "V5      1.000000e+00 -6.934789e-14  1.573956e-11 -2.038243e-12 -1.000756e-12   \n",
            "V6     -6.934789e-14  1.000000e+00 -2.798968e-12 -5.446480e-13  2.036743e-12   \n",
            "V7      1.573956e-11 -2.798968e-12  1.000000e+00  5.528803e-12  5.088082e-13   \n",
            "V8     -2.038243e-12 -5.446480e-13  5.528803e-12  1.000000e+00 -2.243172e-12   \n",
            "V9     -1.000756e-12  2.036743e-12  5.088082e-13 -2.243172e-12  1.000000e+00   \n",
            "V10    -7.200329e-13  7.429770e-13  1.674650e-12 -1.660630e-12  1.185391e-12   \n",
            "V11    -5.928181e-13  1.014893e-12 -8.525291e-13  1.296877e-12 -3.970652e-13   \n",
            "V12     1.812994e-12 -9.265590e-13 -2.826770e-13 -3.860109e-13 -1.904908e-12   \n",
            "V13    -7.021996e-14  1.484679e-12 -8.171731e-13  7.722897e-13  8.754859e-13   \n",
            "V14    -1.113015e-13 -1.212766e-12  2.038217e-12 -2.596182e-12 -1.271311e-12   \n",
            "V15    -1.593594e-12 -1.053548e-12  1.074440e-12  1.648898e-12  8.628709e-13   \n",
            "V16    -1.619090e-14  1.374197e-12 -1.478776e-12 -1.830899e-12  1.239835e-12   \n",
            "V17     1.713794e-13  7.431528e-13 -1.231314e-12  7.025405e-13 -1.450585e-12   \n",
            "V18     1.101433e-12  6.859871e-13 -4.281952e-13  1.499555e-12  7.186934e-13   \n",
            "V19     5.956033e-13  1.148589e-12 -3.742188e-12  1.988417e-12 -8.786777e-13   \n",
            "V20    -1.728728e-13 -2.382062e-12  8.068665e-12 -1.884661e-13  1.270200e-12   \n",
            "V21    -1.751796e-12  1.476858e-12  2.788246e-12 -4.022440e-12  3.040326e-12   \n",
            "V22     7.095269e-13 -1.144797e-12 -8.133209e-13 -2.679560e-12 -7.467526e-13   \n",
            "V23     3.616075e-12 -1.527842e-12 -4.293094e-12  9.013064e-13 -1.011003e-12   \n",
            "V24    -2.808776e-13  1.551854e-12 -2.553518e-12 -1.074365e-12  8.579072e-13   \n",
            "V25     1.451126e-12 -2.723707e-12 -7.406970e-13 -3.268979e-12 -1.590341e-12   \n",
            "V26    -1.896141e-13  3.351239e-12 -4.476467e-12  1.043839e-12 -7.723547e-13   \n",
            "V27    -2.124559e-12  1.481307e-12 -1.328637e-11 -3.499804e-12  2.428930e-12   \n",
            "V28     1.010196e-11 -6.069227e-13  2.958679e-13  1.866598e-12 -1.406856e-12   \n",
            "Amount -3.863563e-01  2.159812e-01  3.973113e-01 -1.030791e-01 -4.424560e-02   \n",
            "Class  -9.497430e-02 -4.364316e-02 -1.872566e-01  1.987512e-02 -9.773269e-02   \n",
            "\n",
            "        ...           V21           V22           V23           V24  \\\n",
            "Time    ...  4.473573e-02  1.440591e-01  5.114236e-02 -1.618187e-02   \n",
            "V1      ... -3.276238e-12  2.281863e-12 -2.969746e-12 -1.029876e-12   \n",
            "V2      ...  2.280202e-12 -2.548560e-13 -4.856120e-12  6.431308e-13   \n",
            "V3      ...  6.736294e-13 -8.909339e-13  4.147209e-12  3.407636e-12   \n",
            "V4      ... -2.696370e-12  4.347776e-13 -4.160969e-12 -2.368743e-12   \n",
            "V5      ... -1.751796e-12  7.095269e-13  3.616075e-12 -2.808776e-13   \n",
            "V6      ...  1.476858e-12 -1.144797e-12 -1.527842e-12  1.551854e-12   \n",
            "V7      ...  2.788246e-12 -8.133209e-13 -4.293094e-12 -2.553518e-12   \n",
            "V8      ... -4.022440e-12 -2.679560e-12  9.013064e-13 -1.074365e-12   \n",
            "V9      ...  3.040326e-12 -7.467526e-13 -1.011003e-12  8.579072e-13   \n",
            "V10     ... -5.547700e-13 -1.320186e-13  1.173332e-12  6.405710e-13   \n",
            "V11     ...  1.100352e-13 -5.644168e-14  1.724963e-12 -1.162239e-12   \n",
            "V12     ...  8.106835e-13 -2.346533e-12 -6.878556e-13 -2.911084e-12   \n",
            "V13     ... -2.037258e-12 -5.491535e-13  3.508022e-12  1.225112e-13   \n",
            "V14     ... -4.557223e-13  2.572021e-12  8.288666e-13 -3.382145e-12   \n",
            "V15     ...  5.921902e-13 -4.115704e-13 -9.846654e-13 -3.256310e-12   \n",
            "V16     ... -1.067918e-12  2.009490e-12  4.057311e-13 -4.061029e-13   \n",
            "V17     ...  1.793607e-12  2.280366e-13 -9.948639e-13 -2.073066e-12   \n",
            "V18     ... -2.185508e-12  1.392636e-12 -2.160673e-12  4.303958e-12   \n",
            "V19     ... -3.315774e-13  7.050020e-14 -7.118335e-13  1.326310e-12   \n",
            "V20     ... -3.892661e-12  1.632957e-12 -1.019668e-11  1.267519e-12   \n",
            "V21     ...  1.000000e+00 -3.415801e-12  1.066923e-12  2.350293e-12   \n",
            "V22     ... -3.415801e-12  1.000000e+00 -9.443573e-13 -1.123546e-12   \n",
            "V23     ...  1.066923e-12 -9.443573e-13  1.000000e+00  2.354049e-12   \n",
            "V24     ...  2.350293e-12 -1.123546e-12  2.354049e-12  1.000000e+00   \n",
            "V25     ... -3.120502e-12  1.968449e-12 -3.751334e-12 -3.917943e-12   \n",
            "V26     ...  8.463789e-13 -1.013828e-12 -1.002379e-12  1.604779e-12   \n",
            "V27     ... -8.527973e-13 -1.726653e-13  9.199153e-12  1.554565e-12   \n",
            "V28     ...  4.256994e-12  5.948423e-12  3.819775e-12  1.380805e-11   \n",
            "Amount  ...  1.059989e-01 -6.480065e-02 -1.126326e-01  5.146217e-03   \n",
            "Class   ...  4.041338e-02  8.053175e-04 -2.685156e-03 -7.220907e-03   \n",
            "\n",
            "                 V25           V26           V27           V28    Amount  \\\n",
            "Time   -2.330828e-01 -4.140710e-02 -5.134591e-03 -9.412688e-03 -0.010596   \n",
            "V1      1.144179e-12  1.835263e-12  7.624804e-12 -9.769215e-13 -0.227709   \n",
            "V2     -9.423730e-13 -4.129100e-13 -9.856545e-13  2.525513e-12 -0.531409   \n",
            "V3      5.712956e-13 -2.577274e-12 -5.041444e-12  5.189109e-12 -0.210880   \n",
            "V4      1.619944e-12 -3.043100e-13 -1.456066e-12 -2.832372e-12  0.098732   \n",
            "V5      1.451126e-12 -1.896141e-13 -2.124559e-12  1.010196e-11 -0.386356   \n",
            "V6     -2.723707e-12  3.351239e-12  1.481307e-12 -6.069227e-13  0.215981   \n",
            "V7     -7.406970e-13 -4.476467e-12 -1.328637e-11  2.958679e-13  0.397311   \n",
            "V8     -3.268979e-12  1.043839e-12 -3.499804e-12  1.866598e-12 -0.103079   \n",
            "V9     -1.590341e-12 -7.723547e-13  2.428930e-12 -1.406856e-12 -0.044246   \n",
            "V10     2.794979e-12 -2.738577e-13  1.552492e-12  5.116568e-12 -0.101502   \n",
            "V11    -1.351430e-12  2.718291e-12 -3.950227e-12 -4.247931e-12  0.000104   \n",
            "V12     1.102899e-12  2.808714e-13  5.953998e-13 -7.428113e-12 -0.009542   \n",
            "V13    -1.513549e-12 -2.008364e-12  4.975659e-12 -6.777880e-12  0.005293   \n",
            "V14     8.299871e-13 -3.304576e-13 -2.447674e-12 -1.700091e-12  0.033751   \n",
            "V15    -1.725436e-12  5.478951e-13 -4.690702e-12 -4.214967e-12 -0.002986   \n",
            "V16     7.626529e-13 -1.323365e-12  7.022747e-12  5.737097e-13 -0.003910   \n",
            "V17     4.514159e-12  2.940618e-12 -1.324408e-12  1.854033e-12  0.007309   \n",
            "V18     5.432404e-13 -1.810692e-12 -4.949670e-12  4.113104e-12  0.035650   \n",
            "V19     9.270702e-13  2.412082e-12 -2.201365e-12  3.450583e-12 -0.056151   \n",
            "V20    -1.593346e-12  1.469484e-13 -2.996546e-12  6.123479e-12  0.339403   \n",
            "V21    -3.120502e-12  8.463789e-13 -8.527973e-13  4.256994e-12  0.105999   \n",
            "V22     1.968449e-12 -1.013828e-12 -1.726653e-13  5.948423e-12 -0.064801   \n",
            "V23    -3.751334e-12 -1.002379e-12  9.199153e-12  3.819775e-12 -0.112633   \n",
            "V24    -3.917943e-12  1.604779e-12  1.554565e-12  1.380805e-11  0.005146   \n",
            "V25     1.000000e+00  2.111834e-12 -6.220008e-13 -8.597190e-12 -0.047837   \n",
            "V26     2.111834e-12  1.000000e+00  2.374854e-12 -1.036858e-11 -0.003208   \n",
            "V27    -6.220008e-13  2.374854e-12  1.000000e+00 -4.441112e-12  0.028825   \n",
            "V28    -8.597190e-12 -1.036858e-11 -4.441112e-12  1.000000e+00  0.010258   \n",
            "Amount -4.783686e-02 -3.208037e-03  2.882546e-02  1.025822e-02  1.000000   \n",
            "Class   3.307706e-03  4.455398e-03  1.757973e-02  9.536041e-03  0.005632   \n",
            "\n",
            "           Class  \n",
            "Time   -0.012323  \n",
            "V1     -0.101347  \n",
            "V2      0.091289  \n",
            "V3     -0.192961  \n",
            "V4      0.133447  \n",
            "V5     -0.094974  \n",
            "V6     -0.043643  \n",
            "V7     -0.187257  \n",
            "V8      0.019875  \n",
            "V9     -0.097733  \n",
            "V10    -0.216883  \n",
            "V11     0.154876  \n",
            "V12    -0.260593  \n",
            "V13    -0.004570  \n",
            "V14    -0.302544  \n",
            "V15    -0.004223  \n",
            "V16    -0.196539  \n",
            "V17    -0.326481  \n",
            "V18    -0.111485  \n",
            "V19     0.034783  \n",
            "V20     0.020090  \n",
            "V21     0.040413  \n",
            "V22     0.000805  \n",
            "V23    -0.002685  \n",
            "V24    -0.007221  \n",
            "V25     0.003308  \n",
            "V26     0.004455  \n",
            "V27     0.017580  \n",
            "V28     0.009536  \n",
            "Amount  0.005632  \n",
            "Class   1.000000  \n",
            "\n",
            "[31 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove highly correlated features\n",
        "threshold = 0.7\n",
        "corr_matrix = corr_matrix.abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
        "df.drop(to_drop, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "05CmIl_iKpAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove highly correlated features\n",
        "threshold = 0.7\n",
        "corr_matrix = corr_matrix.abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "# Exclude the 'Class' column from the correlation check\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > threshold) and column != 'Class']\n",
        "df.drop(to_drop, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "5TUFyCeKFi9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop('Class', axis=1), df['Class'], test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "kcmwNa-rGWh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data** **Preprocessing**"
      ],
      "metadata": {
        "id": "4AMVgnWbJv60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Z-l6R5LdH-MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model** **Training**"
      ],
      "metadata": {
        "id": "w8rzzIvmKDuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "uI7XQ9aCIJe_",
        "outputId": "48348108-8096-4910-dab7-3a4028a15f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model** **Evaluation**"
      ],
      "metadata": {
        "id": "w_pEzizWKSJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation\n",
        "y_pred = gnb.predict(X_test_scaled)\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred))\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S04EWdsDIVwb",
        "outputId": "d4395073-5d33-4891-82eb-40aab8c21426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9778273234788104\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99     56864\n",
            "           1       0.06      0.82      0.11        98\n",
            "\n",
            "    accuracy                           0.98     56962\n",
            "   macro avg       0.53      0.90      0.55     56962\n",
            "weighted avg       1.00      0.98      0.99     56962\n",
            "\n",
            "Confusion Matrix:\n",
            "[[55619  1245]\n",
            " [   18    80]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Results and Interpretation**\n",
        "\n",
        "####Based on the above provided classification report and confusion matrix, here is the interpretation:\n",
        "\n",
        "- Precision:\n",
        "  - For class 0 (the majority class), the precision is 1.00, which indicates that when the model predicts a transaction as non-fraudulent, it is correct 100% of the time.\n",
        "  - For class 1 (the minority class), the precision is 0.06, showing that when the model predicts a transaction as fraudulent, it is correct only 6% of the time. This suggests a high rate of false positives for class 1.\n",
        "\n",
        "- Recall:\n",
        "  - The recall for class 0 is 0.98, signifying that the model correctly identifies 98% of the non-fraudulent transactions.\n",
        "  - The recall for class 1 is 0.82, indicating that the model only captures 82% of the fraudulent transactions.\n",
        "\n",
        "- F1-score:\n",
        "  - The F1-score considers both precision and recall. For class 0, it is 0.99, indicating a high balance between precision and recall. However, for class 1, the F1-score is 0.11, suggesting a significant imbalance between precision and recall for the fraudulent transactions.\n",
        "\n",
        "- Support:\n",
        "  - The support refers to the number of samples of the true response that lie in that class.\n",
        "\n",
        "The confusion matrix complements the classification report by providing a visual representation of the model's performance. In this case, it shows that the model accurately predicted a large proportion of the non-fraudulent transactions (55619 out of 56864), but struggled to correctly identify fraudulent transactions, with only 80 out of 98 being correctly classified.\n",
        "\n",
        "Overall, this interpretation indicates that the model performs well in identifying non-fraudulent transactions but has difficulty detecting fraudulent ones, as evidenced by the low precision, recall, and F1-score for class 1. This suggests a potential imbalance in the dataset or a need for further model refinement to improve its performance in detecting fraudulent transactions.  "
      ],
      "metadata": {
        "id": "uOPaMsoDPaKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "feature_importances = rf.feature_importances_\n",
        "print('Feature Importances:')\n",
        "print(feature_importances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fodzfa9yIhvP",
        "outputId": "60a67082-f266-4a8f-ec03-fcd14fc860ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "[0.0134378  0.01526844 0.01236817 0.01787922 0.03010317 0.01323203\n",
            " 0.0130918  0.0269126  0.01223409 0.05530981 0.09203659 0.10056935\n",
            " 0.08591615 0.00985921 0.12084374 0.01096314 0.06189566 0.15280664\n",
            " 0.02419824 0.01204678 0.01371886 0.01509335 0.00955658 0.00889693\n",
            " 0.00950858 0.00949304 0.01945112 0.01161559 0.01052851 0.01116482]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eUW-VBCyYJc7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}